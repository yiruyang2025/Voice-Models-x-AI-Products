
```
# Cell 0: Install Required Packages
!pip install --quiet librosa soundfile numpy scipy scikit-learn matplotlib tensorflow

import os
import random
import numpy as np
import tensorflow as tf

# Set seeds for reproducibility
SEED = 42
os.environ["PYTHONHASHSEED"] = str(SEED)
random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)

# Optional: force TF deterministic behavior
os.environ['TF_DETERMINISTIC_OPS'] = '1'
```


```
# Cell 1: Synthesize Cowbell Audio with Noise Injection
import os
import numpy as np
import soundfile as sf

sr = 48000
duration = 5.0
n_samples = int(sr * duration)
out_dir = "cowbells"
os.makedirs(out_dir, exist_ok=True)

class_specs = {
    "Swiss":   ([(1000, 0.8), (2500, 0.3)], 30),
    "Urban":   ([(400, 0.7),  (1000, 0.4)], 30),
    "Global":  ([(200, 0.6),  (4000, 0.2)], 30),
    "Unknown": ([(300, 0.6),  (600, 0.4), (1200, 0.3)], 30)
}

def synthesize_and_save(label, specs, count):
    folder = os.path.join(out_dir, label)
    os.makedirs(folder, exist_ok=True)
    for i in range(count):
        t = np.linspace(0, duration, n_samples, endpoint=False)
        signal = sum(amp * np.sin(2*np.pi*freq*t) for freq, amp in specs)
        signal *= np.hanning(n_samples)
        # add white noise at random SNR between 10 and 30 dB
        noise = np.random.randn(n_samples)
        snr = np.random.uniform(10, 30)
        noise *= np.std(signal) * (10**(-snr/20))
        sf.write(os.path.join(folder, f"{label.lower()}_{i}.wav"), signal + noise, sr)

for lbl, (specs, cnt) in class_specs.items():
    synthesize_and_save(lbl, specs, cnt)
```


```
# Cell 2: Load, Pad, and Extract Log-Mel + SpecAugment
import librosa

def load_and_pad(path):
    y, _ = librosa.load(path, sr=sr)
    if len(y) < n_samples:
        y = np.pad(y, (0, n_samples - len(y)))
    else:
        y = y[:n_samples]
    return y

def extract_log_mel(y):
    S = librosa.feature.melspectrogram(
        y=y, sr=sr, n_fft=2048, hop_length=512, n_mels=64
    )
    return librosa.power_to_db(S, ref=np.max)

def spec_augment(mel, F=10, T=20, n_masks=1):
    m = mel.copy()
    M, L = m.shape
    for _ in range(n_masks):
        f0 = np.random.randint(0, M - F)
        t0 = np.random.randint(0, L - T)
        m[f0:f0+F, :] = 0
        m[:, t0:t0+T] = 0
    return m

X, y = [], []
labels = sorted(os.listdir(out_dir))

for idx, lbl in enumerate(labels):
    folder = os.path.join(out_dir, lbl)
    for fn in os.listdir(folder):
        if not fn.endswith(".wav"):
            continue
        y_wave = load_and_pad(os.path.join(folder, fn))
        # 50% chance of spec augment
        mel = extract_log_mel(y_wave)
        if np.random.rand() < 0.5:
            mel = spec_augment(mel)
        X.append(mel)
        y.append(idx)

X = np.stack(X)[..., np.newaxis]
y = np.array(y)
```

```
# Cell 3: Stratified 70/15/15 Split + One-Hot
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical

X_tv, X_test, y_tv, y_test = train_test_split(
    X, y, test_size=0.15, stratify=y, random_state=42
)
X_train, X_val, y_train, y_val = train_test_split(
    X_tv, y_tv, test_size=0.15, stratify=y_tv, random_state=42
)

y_train_cat = to_categorical(y_train, num_classes=len(labels))
y_val_cat   = to_categorical(y_val,   num_classes=len(labels))
y_test_cat  = to_categorical(y_test,  num_classes=len(labels))

print(f"Train/Val/Test: {len(X_train)}/{len(X_val)}/{len(X_test)}")
```


```
Train/Val/Test: 765/136/159
```

```
# Cell 4: Build Lightweight CNN + 1×1 Adapter + Regularization
from tensorflow.keras import layers, models, regularizers, optimizers

inp = layers.Input(shape=X_train.shape[1:])
# 1×1 adapter
x = layers.Conv2D(4, (1,1), activation="relu", padding="same",
                  kernel_regularizer=regularizers.l2(1e-4))(inp)
# two 3×3 Conv+Pool blocks
x = layers.Conv2D(8, (3,3), activation="relu", padding="same",
                  kernel_regularizer=regularizers.l2(1e-4))(x)
x = layers.MaxPool2D((2,2))(x)
x = layers.BatchNormalization()(x)

x = layers.Conv2D(16, (3,3), activation="relu", padding="same",
                  kernel_regularizer=regularizers.l2(1e-4))(x)
x = layers.MaxPool2D((2,2))(x)
x = layers.BatchNormalization()(x)

# Global Average Pooling + classifier
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dropout(0.3)(x)
out = layers.Dense(len(labels), activation="softmax",
                   kernel_regularizer=regularizers.l2(1e-4))(x)

model = models.Model(inputs=inp, outputs=out)
model.compile(
    optimizer=optimizers.Adam(1e-3),
    loss="categorical_crossentropy",
    metrics=["accuracy"]
)
model.summary()
```

```
Model: "functional_22"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ input_layer_23 (InputLayer)     │ (None, 64, 469, 1)     │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_68 (Conv2D)              │ (None, 64, 469, 4)     │             8 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_69 (Conv2D)              │ (None, 64, 469, 8)     │           296 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max_pooling2d_46 (MaxPooling2D) │ (None, 32, 234, 8)     │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ batch_normalization_28          │ (None, 32, 234, 8)     │            32 │
│ (BatchNormalization)            │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_70 (Conv2D)              │ (None, 32, 234, 16)    │         1,168 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max_pooling2d_47 (MaxPooling2D) │ (None, 16, 117, 16)    │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ batch_normalization_29          │ (None, 16, 117, 16)    │            64 │
│ (BatchNormalization)            │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ global_average_pooling2d_22     │ (None, 16)             │             0 │
│ (GlobalAveragePooling2D)        │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dropout_17 (Dropout)            │ (None, 16)             │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_27 (Dense)                │ (None, 4)              │            68 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 1,636 (6.39 KB)
 Trainable params: 1,588 (6.20 KB)
 Non-trainable params: 48 (192.00 B)
```

```
from tensorflow.keras.utils import plot_model
from IPython.display import Image, display

# Save model architecture diagram
plot_model(model, to_file="model_architecture.png", show_shapes=True, show_layer_names=True)

# Display the image
display(Image(filename="model_architecture.png"))
```

```
```

```
# Cell 5: Compute Class Weights, Train with EarlyStopping
from sklearn.utils import class_weight
from tensorflow.keras.callbacks import EarlyStopping

# weights: Swiss=1, Urban=5, Other(Global+Unknown)=8
cw = class_weight.compute_class_weight(
    'balanced', classes=np.arange(len(labels)), y=y_train
)
cw_dict = dict(enumerate(cw))
cw_dict[labels.index("Swiss")]  = 1.0
cw_dict[labels.index("Urban")]  = 5.0
for other in ["Global", "Unknown"]:
    cw_dict[labels.index(other)] = 8.0

early = EarlyStopping(
    monitor="val_accuracy", mode="max", patience=8,
    restore_best_weights=True, verbose=1
)

history = model.fit(
    X_train, y_train_cat,
    validation_data=(X_val, y_val_cat),
    epochs=60,
    batch_size=32,
    class_weight=cw_dict,
    callbacks=[early],
    verbose=2
)
```


```
Epoch 1/60
24/24 - 6s - 258ms/step - accuracy: 0.4052 - loss: 6.2515 - val_accuracy: 0.3676 - val_loss: 1.2904
Epoch 2/60
24/24 - 4s - 181ms/step - accuracy: 0.5817 - loss: 5.4544 - val_accuracy: 0.4265 - val_loss: 1.2533
Epoch 3/60
24/24 - 4s - 172ms/step - accuracy: 0.6183 - loss: 4.8862 - val_accuracy: 0.3235 - val_loss: 1.1083
Epoch 4/60
24/24 - 4s - 159ms/step - accuracy: 0.6471 - loss: 4.3749 - val_accuracy: 0.4485 - val_loss: 1.1101
Epoch 5/60
24/24 - 4s - 161ms/step - accuracy: 0.6876 - loss: 4.0147 - val_accuracy: 0.6029 - val_loss: 0.9610
Epoch 6/60
24/24 - 4s - 158ms/step - accuracy: 0.6915 - loss: 3.7419 - val_accuracy: 0.8309 - val_loss: 1.0172
Epoch 7/60
24/24 - 4s - 156ms/step - accuracy: 0.6824 - loss: 3.4911 - val_accuracy: 0.6471 - val_loss: 0.9444
Epoch 8/60
24/24 - 4s - 162ms/step - accuracy: 0.7150 - loss: 3.2070 - val_accuracy: 0.6397 - val_loss: 0.9577
Epoch 9/60
24/24 - 4s - 165ms/step - accuracy: 0.7098 - loss: 3.0157 - val_accuracy: 0.3088 - val_loss: 1.2412
Epoch 10/60
24/24 - 4s - 155ms/step - accuracy: 0.7281 - loss: 2.8508 - val_accuracy: 0.5882 - val_loss: 0.9905
Epoch 11/60
24/24 - 4s - 161ms/step - accuracy: 0.7190 - loss: 2.7658 - val_accuracy: 0.5588 - val_loss: 0.9817
Epoch 12/60
24/24 - 4s - 156ms/step - accuracy: 0.7412 - loss: 2.5968 - val_accuracy: 0.5147 - val_loss: 0.9514
Epoch 13/60
24/24 - 4s - 159ms/step - accuracy: 0.7699 - loss: 2.4086 - val_accuracy: 0.5809 - val_loss: 0.8626
Epoch 14/60
24/24 - 4s - 162ms/step - accuracy: 0.7660 - loss: 2.3818 - val_accuracy: 0.7721 - val_loss: 0.8013
Epoch 14: early stopping
Restoring model weights from the end of the best epoch: 6.
```


```
# Cell 6: Evaluate via Balanced Accuracy & Confusion Matrix
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix, balanced_accuracy_score
import tensorflow as tf

@tf.function(reduce_retracing=True)
def predict_fn(x):
    return model(x, training=False)

y_pred = predict_fn(X_test).numpy().argmax(axis=1)
y_true = y_test

print("\nClassification Report:\n")
print(classification_report(y_true, y_pred, target_names=labels))
print(f"Balanced accuracy: {balanced_accuracy_score(y_true, y_pred):.3f}")

cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(4,3))
plt.imshow(cm, cmap="Blues")
plt.xticks(range(len(labels)), labels, rotation=45)
plt.yticks(range(len(labels)), labels)
plt.colorbar()
plt.title("Confusion Matrix")
plt.tight_layout()
plt.show()
```


```
Classification Report:

              precision    recall  f1-score   support

      Global       0.76      0.96      0.85        50
       Swiss       0.72      0.86      0.79        49
     Unknown       0.44      0.50      0.47         8
       Urban       1.00      0.56      0.72        52

    accuracy                           0.77       159
   macro avg       0.73      0.72      0.71       159
weighted avg       0.81      0.77      0.77       159

Balanced accuracy: 0.719
```

```
# Cell 7: Demo Interface via File Upload
import numpy as np
import librosa
import ipywidgets as widgets
import matplotlib.pyplot as plt
from IPython.display import display, Audio

# Upload widget for multiple files
uploader = widgets.FileUpload(
    accept='.wav,.mp3',
    multiple=True,
    description='Upload Cowbell Audio'
)
output = widgets.Output()

def preprocess_audio(data: bytes):
    # load bytes buffer with librosa
    import io
    y, _ = librosa.load(io.BytesIO(data), sr=sr, mono=True)
    if len(y) < n_samples:
        y = np.pad(y, (0, n_samples - len(y)))
    else:
        y = y[:n_samples]
    mel = extract_log_mel(y)
    return mel[..., np.newaxis]

def on_upload_change(change):
    with output:
        output.clear_output()
        for filename, fileinfo in uploader.value.items():
            print(f"=== {filename} ===")
            display(Audio(data=fileinfo['content'], rate=sr))
            mel = preprocess_audio(fileinfo['content'])
            probs = predict_fn(mel[np.newaxis, ...]).numpy()[0]
            for cls, p in zip(labels, probs):
                print(f"{cls}: {p:.4f}")
            # bar plot
            plt.figure(figsize=(5,2))
            plt.bar(labels, probs)
            plt.ylim(0,1)
            plt.title(f"Probabilities for {filename}")
            plt.xticks(rotation=45)
            plt.show()

uploader.observe(on_upload_change, names='value')
display(uploader, output)
```


```

```


```

```




```
@article{yiruyang2025,
  title={Pick Veo 3},
  author={Yiru Yang},
  journal={arXiv preprint arXiv:2505.xxxxx},
  year={2025}
}
```

