

```
# Cell 0: Install Required Packages
!pip install --quiet librosa soundfile numpy scipy scikit-learn matplotlib tensorflow

import os
import random
import numpy as np
import pandas as pd
from glob import glob
import tensorflow as tf
if not hasattr(np, 'complex'):
    np.complex = np.complex128  # or just `complex`
import librosa
import librosa.display
import matplotlib.pyplot as plt
from IPython.display import display, Markdown

# Set seeds for reproducibility
SEED = 42
os.environ["PYTHONHASHSEED"] = str(SEED)
random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)

# Optional: force TF deterministic behavior
os.environ['TF_DETERMINISTIC_OPS'] = '1'
```



```
# Cell 1: Synthesize Cowbell Audio with Noise Injection
import soundfile as sf

sr = 48000
duration = 5.0
n_samples = int(sr * duration)
out_dir = "cowbells"
os.makedirs(out_dir, exist_ok=True)

class_specs = {
    "Swiss":   ([(1000, 0.8), (2500, 0.3)], 50),
    "Urban":   ([(400, 0.7),  (1000, 0.4)], 50),
    "Global":  ([(200, 0.6),  (4000, 0.2)], 50),
    "Unknown": ([(300, 0.6),  (600, 0.4), (1200, 0.3)], 50)
}

def synthesize_and_save(label, specs, count):
    folder = os.path.join(out_dir, label)
    os.makedirs(folder, exist_ok=True)
    for i in range(count):
        t = np.linspace(0, duration, n_samples, endpoint=False)
        signal = sum(amp * np.sin(2*np.pi*freq*t) for freq, amp in specs)
        signal *= np.hanning(n_samples)
        # add white noise at random SNR between 10 and 30 dB
        noise = np.random.randn(n_samples)
        snr = np.random.uniform(10, 30)
        noise *= np.std(signal) * (10**(-snr/20))
        sf.write(os.path.join(folder, f"{label.lower()}_{i}.wav"), signal + noise, sr)

for lbl, (specs, cnt) in class_specs.items():
    synthesize_and_save(lbl, specs, cnt)
```

```
# Cell 2: Load, Pad, and Extract Log-Mel + SpecAugment
import librosa

def load_and_pad(path):
    y, _ = librosa.load(path, sr=sr)
    if len(y) < n_samples:
        y = np.pad(y, (0, n_samples - len(y)))
    else:
        y = y[:n_samples]
    return y

def extract_log_mel(y):
    S = librosa.feature.melspectrogram(
        y=y, sr=sr, n_fft=2048, hop_length=512, n_mels=64
    )
    return librosa.power_to_db(S, ref=np.max)

def spec_augment(mel, F=10, T=20, n_masks=1):
    m = mel.copy()
    M, L = m.shape
    for _ in range(n_masks):
        f0 = np.random.randint(0, M - F)
        t0 = np.random.randint(0, L - T)
        m[f0:f0+F, :] = 0
        m[:, t0:t0+T] = 0
    return m

X, y = [], []
labels = sorted(os.listdir(out_dir))

for idx, lbl in enumerate(labels):
    folder = os.path.join(out_dir, lbl)
    for fn in os.listdir(folder):
        if not fn.endswith(".wav"):
            continue
        y_wave = load_and_pad(os.path.join(folder, fn))
        # 50% chance of spec augment
        mel = extract_log_mel(y_wave)
        if np.random.rand() < 0.5:
            mel = spec_augment(mel)
        X.append(mel)
        y.append(idx)

X = np.stack(X)[..., np.newaxis]
y = np.array(y)
```



```
# Cell 2.5: Visualize Original and Augmented Audio Spectrograms + Summary Table

# ---------- Helper: Estimate semitone shift from a reference frequency ----------
def estimate_semitone_shift(f_actual, f_ref):
    if f_ref == 0 or f_actual == 0:
        return 0
    return np.round(12 * np.log2(f_actual / f_ref), 1)

# ---------- Function: Annotated log-mel spectrogram ----------
def plot_log_mel(filepath, title="Sample", reference_freq=None):
    y, sr = librosa.load(filepath, sr=48000)
    if len(y) == 0:
        print(f"[Warning] Empty audio: {filepath}")
        return
    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=64)
    S_dB = librosa.power_to_db(S, ref=np.max)
    mel_freqs = librosa.mel_frequencies(n_mels=64)

    f0_idx = np.argmax(S.mean(axis=1))
    f0_freq = mel_freqs[f0_idx]

    # Estimate shift from reference frequency
    semitone_shift = estimate_semitone_shift(f0_freq, reference_freq) if reference_freq else None

    plt.figure(figsize=(9, 4))
    librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel', cmap='magma')
    plt.colorbar(format="%+2.0f dB")
    plt.title(title, fontsize=13)

    # Fundamental
    plt.axhline(y=f0_freq, color='cyan', linestyle='--', linewidth=1.2)
    plt.text(0.05, f0_freq + 200, f"Fundamental: {int(f0_freq)} Hz", color='cyan', fontsize=9)

    # Overtones
    overtone_start = min(f0_freq * 2, mel_freqs[-1])
    overtone_end = min(f0_freq * 2 + 2000, mel_freqs[-1])
    plt.axhspan(overtone_start, overtone_end, color='yellow', alpha=0.3)
    plt.text(0.05, overtone_start + 400, "Overtones / Resonance", color='darkorange', fontsize=9)

    # Caption
    plt.text(0, -5, "Spectral distribution varies across classes – classification basis",
             fontsize=8, color='white', bbox=dict(facecolor='black', alpha=0.6))

    # Optional semitone display
    if semitone_shift is not None:
        plt.text(0.7, 0.85 * mel_freqs[-1],
                 f"Est. Pitch Shift: {semitone_shift:+.1f} semitones",
                 fontsize=9, color='white', bbox=dict(facecolor='gray', alpha=0.5))

    plt.tight_layout()
    plt.show()
    return f0_freq

# ---------- Original Sample Visualization ----------
base_dir = "cowbells"
summary = {}
reference_freqs = {}

if os.path.exists(base_dir):
    for cls in sorted(os.listdir(base_dir)):
        cls_path = os.path.join(base_dir, cls)
        if not os.path.isdir(cls_path): continue
        wavs = glob(os.path.join(cls_path, "*.wav"))
        summary[(cls, 'Original')] = len(wavs)
        if wavs:
            reference_freqs[cls] = plot_log_mel(wavs[0], title=f"Original - {cls}")
        else:
            print(f"[Empty] No .wav files in {cls_path}")
else:
    print("[Skipped] Folder not found: cowbells")

# ---------- In-Memory Augmented Samples ----------
shown = set()
plt.figure(figsize=(12, 8))

for i in range(len(X)):
    label_idx = y[i]
    label_name = sorted(os.listdir(base_dir))[label_idx]
    if label_name in shown: continue

    plt.subplot(2, 2, len(shown) + 1)
    mel = X[i].squeeze()
    librosa.display.specshow(mel, sr=48000, x_axis='time', y_axis='mel', cmap='magma')
    plt.title(f"Augmented - {label_name}", fontsize=11)
    plt.colorbar(format="%+2.0f dB")

    mel_freqs = librosa.mel_frequencies(n_mels=64)
    f0_idx = np.argmax(mel.mean(axis=1))
    f0_freq = mel_freqs[f0_idx]
    semitone_shift = estimate_semitone_shift(f0_freq, reference_freqs.get(label_name, f0_freq))

    # Annotations
    plt.axhline(y=f0_freq, color='cyan', linestyle='--', linewidth=1)
    plt.text(0.05, f0_freq + 200, f"F0: {int(f0_freq)} Hz", color='cyan', fontsize=8)
    overtone_start = min(f0_freq * 2, mel_freqs[-1])
    overtone_end = min(f0_freq * 2 + 2000, mel_freqs[-1])
    plt.axhspan(overtone_start, overtone_end, color='yellow', alpha=0.3)
    plt.text(0.05, overtone_start + 400, "Overtones", color='darkorange', fontsize=8)
    plt.text(0.05, 0, f"Shift: {semitone_shift:+.1f} semitones",
             fontsize=8, color='white', bbox=dict(facecolor='gray', alpha=0.5))

    plt.tight_layout()
    shown.add(label_name)
    if len(shown) == 4: break

plt.suptitle("Augmented Samples - Log-Mel Spectrograms", fontsize=14)
plt.show()
```


```
# Cell 3: Stratified 70/15/15 Split + One-Hot
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical

# Step 1: Split 15% Test set from total
X_remain, X_test, y_remain, y_test = train_test_split(
    X, y, test_size=0.15, stratify=y, random_state=42
)

# Step 2: From remaining 85%, split 15/85 ≈ 0.176 into validation
X_train, X_val, y_train, y_val = train_test_split(
    X_remain, y_remain, test_size=0.176, stratify=y_remain, random_state=42
)

# One-hot encode the labels
y_train_cat = to_categorical(y_train, num_classes=len(labels))
y_val_cat   = to_categorical(y_val,   num_classes=len(labels))
y_test_cat  = to_categorical(y_test,  num_classes=len(labels))

print(f"Train/Val/Test: {len(X_train)}/{len(X_val)}/{len(X_test)}")
```



```
Train/Val/Test: 560/120/120
```


```
# Cell 4: Build Lightweight CNN + 1×1 Adapter + Regularization (Improved)
from tensorflow.keras import layers, models, regularizers, optimizers, initializers

inp = layers.Input(shape=X_train.shape[1:])

# 1×1 adapter
x = layers.Conv2D(4, (1, 1), padding="same",
                  kernel_initializer='he_normal',
                  kernel_regularizer=regularizers.l2(1e-4))(inp)
x = layers.LeakyReLU()(x)

# First 3×3 Conv + Pool block
x = layers.Conv2D(8, (3, 3), padding="same",
                  kernel_initializer='he_normal',
                  kernel_regularizer=regularizers.l2(1e-4))(x)
x = layers.LeakyReLU()(x)
x = layers.MaxPooling2D((2, 2))(x)
x = layers.BatchNormalization()(x)

# Second 3×3 Conv + Pool block
x = layers.Conv2D(16, (3, 3), padding="same",
                  kernel_initializer='he_normal',
                  kernel_regularizer=regularizers.l2(1e-4))(x)
x = layers.LeakyReLU()(x)
x = layers.MaxPooling2D((2, 2))(x)
x = layers.BatchNormalization()(x)

# Global Average Pooling + classifier
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dropout(0.4)(x)
out = layers.Dense(len(labels), activation="softmax",
                   kernel_regularizer=regularizers.l2(1e-4))(x)

model = models.Model(inputs=inp, outputs=out)
model.compile(
    optimizer=optimizers.Adam(1e-3),
    loss="categorical_crossentropy",
    metrics=["accuracy"]
)
model.summary()
```



```
Model: "functional_9"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ input_layer_9 (InputLayer)      │ (None, 64, 469, 1)     │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_27 (Conv2D)              │ (None, 64, 469, 4)     │             8 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ leaky_re_lu_12 (LeakyReLU)      │ (None, 64, 469, 4)     │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_28 (Conv2D)              │ (None, 64, 469, 8)     │           296 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ leaky_re_lu_13 (LeakyReLU)      │ (None, 64, 469, 8)     │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max_pooling2d_18 (MaxPooling2D) │ (None, 32, 234, 8)     │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ batch_normalization_18          │ (None, 32, 234, 8)     │            32 │
│ (BatchNormalization)            │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_29 (Conv2D)              │ (None, 32, 234, 16)    │         1,168 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ leaky_re_lu_14 (LeakyReLU)      │ (None, 32, 234, 16)    │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max_pooling2d_19 (MaxPooling2D) │ (None, 16, 117, 16)    │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ batch_normalization_19          │ (None, 16, 117, 16)    │            64 │
│ (BatchNormalization)            │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ global_average_pooling2d_9      │ (None, 16)             │             0 │
│ (GlobalAveragePooling2D)        │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dropout_9 (Dropout)             │ (None, 16)             │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_9 (Dense)                 │ (None, 4)              │            68 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 1,636 (6.39 KB)
 Trainable params: 1,588 (6.20 KB)
 Non-trainable params: 48 (192.00 B)
```


```
# Cell 4.5: Model Structure Visualization (Simplified & Clear)

from tensorflow.keras.utils import plot_model
from IPython.display import Image, display

# Save a simplified and clear model diagram
plot_model(
    model,
    to_file="model_architecture.png",
    show_shapes=True,          # Show tensor shapes for clarity
    show_layer_names=True,     # Show layer names
    rankdir='TB',              # Top-to-Bottom layout (better for compact models)
    expand_nested=False,       # No nested structure (flattened layout)
    dpi=120                    # Higher resolution for better rendering
)

# Display the image inline
display(Image(filename="model_architecture.png"))
```



```
# Cell 5: Compute Class Weights, Train with EarlyStopping and ReduceLROnPlateau
from sklearn.utils import class_weight
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback
from tensorflow.keras import backend as K

# Step 1: Compute class weights based on label imbalance
cw = class_weight.compute_class_weight(
    class_weight='balanced',
    classes=np.arange(len(labels)),
    y=y_train
)
cw_dict = dict(enumerate(cw))

# Manual tuning for important classes
cw_dict[labels.index("Swiss")] = 4.0
cw_dict[labels.index("Urban")] = 3.0
cw_dict[labels.index("Global")] = 2.0
cw_dict[labels.index("Unknown")] = 1.5
print("Adjusted class weights:", cw_dict)

# Step 2: Callback to log learning rate per epoch
class PrintLR(Callback):
    def on_epoch_end(self, epoch, logs=None):
        lr = self.model.optimizer.learning_rate.numpy()
        print(f"Learning rate at epoch {epoch + 1}: {lr:.6f}")

# Step 3: EarlyStopping based on validation loss
early = EarlyStopping(
    monitor="val_loss",
    mode="min",
    patience=8,
    restore_best_weights=True,
    verbose=1
)

# Step 4: Reduce learning rate on plateau
reduce_lr = ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.6,
    patience=4,
    min_lr=5e-5,
    verbose=1
)

# Step 5: Train the model
history = model.fit(
    X_train, y_train_cat,
    validation_data=(X_val, y_val_cat),
    epochs=60,
    batch_size=32,
    class_weight=cw_dict,
    callbacks=[early, reduce_lr, PrintLR()],
    verbose=2
)

# Step 6: Print best epoch's performance
best_epoch = np.argmin(history.history['val_loss'])
print(f"\nBest epoch: {best_epoch + 1}")
print(f"Training  - Accuracy: {history.history['accuracy'][best_epoch]:.4f}, Loss: {history.history['loss'][best_epoch]:.4f}")
print(f"Validation - Accuracy: {history.history['val_accuracy'][best_epoch]:.4f}, Loss: {history.history['val_loss'][best_epoch]:.4f}")
```


```
Adjusted class weights: {0: 2.0, 1: 4.0, 2: 1.5, 3: 3.0}
Epoch 1/60
Learning rate at epoch 1: 0.001000
18/18 - 8s - 427ms/step - accuracy: 0.2839 - loss: 3.7016 - val_accuracy: 0.2500 - val_loss: 6.2204 - learning_rate: 1.0000e-03
Epoch 2/60
Learning rate at epoch 2: 0.001000
18/18 - 4s - 235ms/step - accuracy: 0.3607 - loss: 3.4316 - val_accuracy: 0.2500 - val_loss: 3.6008 - learning_rate: 1.0000e-03
Epoch 3/60
Learning rate at epoch 3: 0.001000
18/18 - 4s - 241ms/step - accuracy: 0.4536 - loss: 3.1741 - val_accuracy: 0.2500 - val_loss: 2.3359 - learning_rate: 1.0000e-03
Epoch 4/60
Learning rate at epoch 4: 0.001000
18/18 - 4s - 248ms/step - accuracy: 0.5500 - loss: 2.9822 - val_accuracy: 0.2500 - val_loss: 2.0402 - learning_rate: 1.0000e-03
Epoch 5/60
Learning rate at epoch 5: 0.001000
18/18 - 4s - 242ms/step - accuracy: 0.5696 - loss: 2.8722 - val_accuracy: 0.2500 - val_loss: 2.2098 - learning_rate: 1.0000e-03
Epoch 6/60
Learning rate at epoch 6: 0.001000
18/18 - 4s - 235ms/step - accuracy: 0.6089 - loss: 2.7015 - val_accuracy: 0.2500 - val_loss: 1.6539 - learning_rate: 1.0000e-03
Epoch 7/60
Learning rate at epoch 7: 0.001000
18/18 - 5s - 253ms/step - accuracy: 0.6357 - loss: 2.6009 - val_accuracy: 0.2500 - val_loss: 1.6016 - learning_rate: 1.0000e-03
Epoch 8/60
Learning rate at epoch 8: 0.001000
18/18 - 4s - 240ms/step - accuracy: 0.6893 - loss: 2.4271 - val_accuracy: 0.2500 - val_loss: 1.6500 - learning_rate: 1.0000e-03
Epoch 9/60
Learning rate at epoch 9: 0.001000
18/18 - 4s - 233ms/step - accuracy: 0.7268 - loss: 2.3169 - val_accuracy: 0.4667 - val_loss: 1.2238 - learning_rate: 1.0000e-03
Epoch 10/60
Learning rate at epoch 10: 0.001000
18/18 - 5s - 253ms/step - accuracy: 0.7571 - loss: 2.2259 - val_accuracy: 0.5000 - val_loss: 1.0315 - learning_rate: 1.0000e-03
Epoch 11/60
Learning rate at epoch 11: 0.001000
18/18 - 4s - 237ms/step - accuracy: 0.7857 - loss: 2.0859 - val_accuracy: 0.5333 - val_loss: 0.9115 - learning_rate: 1.0000e-03
Epoch 12/60
Learning rate at epoch 12: 0.001000
18/18 - 4s - 249ms/step - accuracy: 0.8482 - loss: 1.9476 - val_accuracy: 0.5083 - val_loss: 0.9845 - learning_rate: 1.0000e-03
Epoch 13/60
Learning rate at epoch 13: 0.001000
18/18 - 4s - 250ms/step - accuracy: 0.8339 - loss: 1.8904 - val_accuracy: 0.4833 - val_loss: 0.9738 - learning_rate: 1.0000e-03
Epoch 14/60
Learning rate at epoch 14: 0.001000
18/18 - 4s - 241ms/step - accuracy: 0.8607 - loss: 1.7673 - val_accuracy: 0.5167 - val_loss: 0.8847 - learning_rate: 1.0000e-03
Epoch 15/60
Learning rate at epoch 15: 0.001000
18/18 - 4s - 239ms/step - accuracy: 0.8839 - loss: 1.6656 - val_accuracy: 0.7417 - val_loss: 0.7571 - learning_rate: 1.0000e-03
Epoch 16/60
Learning rate at epoch 16: 0.001000
18/18 - 4s - 243ms/step - accuracy: 0.8714 - loss: 1.6141 - val_accuracy: 0.7750 - val_loss: 0.7298 - learning_rate: 1.0000e-03
Epoch 17/60
Learning rate at epoch 17: 0.001000
18/18 - 5s - 251ms/step - accuracy: 0.8929 - loss: 1.5336 - val_accuracy: 0.5833 - val_loss: 0.9713 - learning_rate: 1.0000e-03
Epoch 18/60
Learning rate at epoch 18: 0.001000
18/18 - 4s - 241ms/step - accuracy: 0.9143 - loss: 1.4379 - val_accuracy: 0.6417 - val_loss: 0.8814 - learning_rate: 1.0000e-03
Epoch 19/60
Learning rate at epoch 19: 0.001000
18/18 - 4s - 246ms/step - accuracy: 0.9196 - loss: 1.3791 - val_accuracy: 0.5917 - val_loss: 1.0550 - learning_rate: 1.0000e-03
Epoch 20/60

Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.
Learning rate at epoch 20: 0.000600
18/18 - 4s - 241ms/step - accuracy: 0.9339 - loss: 1.2754 - val_accuracy: 0.6833 - val_loss: 0.8808 - learning_rate: 1.0000e-03
Epoch 21/60
Learning rate at epoch 21: 0.000600
18/18 - 5s - 253ms/step - accuracy: 0.9268 - loss: 1.2699 - val_accuracy: 0.4417 - val_loss: 1.0603 - learning_rate: 6.0000e-04
Epoch 22/60
Learning rate at epoch 22: 0.000600
18/18 - 5s - 295ms/step - accuracy: 0.9429 - loss: 1.2241 - val_accuracy: 0.9583 - val_loss: 0.4730 - learning_rate: 6.0000e-04
Epoch 23/60
Learning rate at epoch 23: 0.000600
18/18 - 5s - 256ms/step - accuracy: 0.9214 - loss: 1.1909 - val_accuracy: 0.7250 - val_loss: 0.6159 - learning_rate: 6.0000e-04
Epoch 24/60
Learning rate at epoch 24: 0.000600
18/18 - 5s - 253ms/step - accuracy: 0.9232 - loss: 1.1773 - val_accuracy: 0.7750 - val_loss: 0.5413 - learning_rate: 6.0000e-04
Epoch 25/60
Learning rate at epoch 25: 0.000600
18/18 - 5s - 276ms/step - accuracy: 0.9536 - loss: 1.1194 - val_accuracy: 0.8083 - val_loss: 0.4727 - learning_rate: 6.0000e-04
Epoch 26/60
Learning rate at epoch 26: 0.000600
18/18 - 5s - 257ms/step - accuracy: 0.9411 - loss: 1.1323 - val_accuracy: 0.9917 - val_loss: 0.3758 - learning_rate: 6.0000e-04
Epoch 27/60
Learning rate at epoch 27: 0.000600
18/18 - 4s - 245ms/step - accuracy: 0.9339 - loss: 1.0630 - val_accuracy: 0.9167 - val_loss: 0.4390 - learning_rate: 6.0000e-04
Epoch 28/60
Learning rate at epoch 28: 0.000600
18/18 - 5s - 261ms/step - accuracy: 0.9518 - loss: 1.0353 - val_accuracy: 0.8833 - val_loss: 0.4745 - learning_rate: 6.0000e-04
Epoch 29/60
Learning rate at epoch 29: 0.000600
18/18 - 4s - 247ms/step - accuracy: 0.9429 - loss: 1.0356 - val_accuracy: 0.8167 - val_loss: 0.4124 - learning_rate: 6.0000e-04
Epoch 30/60
Learning rate at epoch 30: 0.000600
18/18 - 5s - 252ms/step - accuracy: 0.9464 - loss: 1.0015 - val_accuracy: 0.9917 - val_loss: 0.3371 - learning_rate: 6.0000e-04
Epoch 31/60
Learning rate at epoch 31: 0.000600
18/18 - 5s - 269ms/step - accuracy: 0.9411 - loss: 0.9864 - val_accuracy: 0.9083 - val_loss: 0.4104 - learning_rate: 6.0000e-04
Epoch 32/60
Learning rate at epoch 32: 0.000600
18/18 - 4s - 248ms/step - accuracy: 0.9446 - loss: 1.0019 - val_accuracy: 0.8417 - val_loss: 0.4546 - learning_rate: 6.0000e-04
Epoch 33/60
Learning rate at epoch 33: 0.000600
18/18 - 4s - 239ms/step - accuracy: 0.9536 - loss: 0.9374 - val_accuracy: 0.8000 - val_loss: 0.4143 - learning_rate: 6.0000e-04
Epoch 34/60
Learning rate at epoch 34: 0.000600
18/18 - 5s - 261ms/step - accuracy: 0.9518 - loss: 0.9071 - val_accuracy: 0.8583 - val_loss: 0.3307 - learning_rate: 6.0000e-04
Epoch 35/60
Learning rate at epoch 35: 0.000600
18/18 - 4s - 240ms/step - accuracy: 0.9464 - loss: 0.9355 - val_accuracy: 0.7500 - val_loss: 0.5839 - learning_rate: 6.0000e-04
Epoch 36/60
Learning rate at epoch 36: 0.000600
18/18 - 4s - 245ms/step - accuracy: 0.9518 - loss: 0.8952 - val_accuracy: 0.9917 - val_loss: 0.2514 - learning_rate: 6.0000e-04
Epoch 37/60
Learning rate at epoch 37: 0.000600
18/18 - 5s - 260ms/step - accuracy: 0.9518 - loss: 0.8346 - val_accuracy: 0.9917 - val_loss: 0.2537 - learning_rate: 6.0000e-04
Epoch 38/60
Learning rate at epoch 38: 0.000600
18/18 - 4s - 249ms/step - accuracy: 0.9429 - loss: 0.8363 - val_accuracy: 0.9917 - val_loss: 0.2221 - learning_rate: 6.0000e-04
Epoch 39/60
Learning rate at epoch 39: 0.000600
18/18 - 4s - 247ms/step - accuracy: 0.9500 - loss: 0.8482 - val_accuracy: 0.7667 - val_loss: 0.5495 - learning_rate: 6.0000e-04
Epoch 40/60
Learning rate at epoch 40: 0.000600
18/18 - 5s - 259ms/step - accuracy: 0.9536 - loss: 0.8316 - val_accuracy: 0.5417 - val_loss: 0.8207 - learning_rate: 6.0000e-04
Epoch 41/60
Learning rate at epoch 41: 0.000600
18/18 - 5s - 257ms/step - accuracy: 0.9482 - loss: 0.8318 - val_accuracy: 1.0000 - val_loss: 0.2245 - learning_rate: 6.0000e-04
Epoch 42/60

Epoch 42: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.
Learning rate at epoch 42: 0.000360
18/18 - 5s - 251ms/step - accuracy: 0.9518 - loss: 0.7811 - val_accuracy: 0.8000 - val_loss: 0.3791 - learning_rate: 6.0000e-04
Epoch 43/60
Learning rate at epoch 43: 0.000360
18/18 - 5s - 263ms/step - accuracy: 0.9464 - loss: 0.7774 - val_accuracy: 0.9917 - val_loss: 0.2426 - learning_rate: 3.6000e-04
Epoch 44/60
Learning rate at epoch 44: 0.000360
18/18 - 5s - 252ms/step - accuracy: 0.9482 - loss: 0.7538 - val_accuracy: 0.9917 - val_loss: 0.2110 - learning_rate: 3.6000e-04
Epoch 45/60
Learning rate at epoch 45: 0.000360
18/18 - 5s - 260ms/step - accuracy: 0.9607 - loss: 0.6924 - val_accuracy: 0.9917 - val_loss: 0.2572 - learning_rate: 3.6000e-04
Epoch 46/60
Learning rate at epoch 46: 0.000360
18/18 - 5s - 270ms/step - accuracy: 0.9482 - loss: 0.7183 - val_accuracy: 0.9917 - val_loss: 0.2638 - learning_rate: 3.6000e-04
Epoch 47/60
Learning rate at epoch 47: 0.000360
18/18 - 5s - 259ms/step - accuracy: 0.9607 - loss: 0.7475 - val_accuracy: 0.9833 - val_loss: 0.2062 - learning_rate: 3.6000e-04
Epoch 48/60
Learning rate at epoch 48: 0.000360
18/18 - 5s - 258ms/step - accuracy: 0.9643 - loss: 0.7094 - val_accuracy: 0.9833 - val_loss: 0.2770 - learning_rate: 3.6000e-04
Epoch 49/60
Learning rate at epoch 49: 0.000360
18/18 - 5s - 273ms/step - accuracy: 0.9536 - loss: 0.6695 - val_accuracy: 0.9917 - val_loss: 0.2257 - learning_rate: 3.6000e-04
Epoch 50/60
Learning rate at epoch 50: 0.000360
18/18 - 4s - 246ms/step - accuracy: 0.9518 - loss: 0.7301 - val_accuracy: 1.0000 - val_loss: 0.1829 - learning_rate: 3.6000e-04
Epoch 51/60
Learning rate at epoch 51: 0.000360
18/18 - 5s - 263ms/step - accuracy: 0.9661 - loss: 0.6877 - val_accuracy: 1.0000 - val_loss: 0.1737 - learning_rate: 3.6000e-04
Epoch 52/60
Learning rate at epoch 52: 0.000360
18/18 - 5s - 260ms/step - accuracy: 0.9482 - loss: 0.7215 - val_accuracy: 0.9917 - val_loss: 0.1728 - learning_rate: 3.6000e-04
Epoch 53/60
Learning rate at epoch 53: 0.000360
18/18 - 4s - 246ms/step - accuracy: 0.9696 - loss: 0.6594 - val_accuracy: 0.9917 - val_loss: 0.2171 - learning_rate: 3.6000e-04
Epoch 54/60
Learning rate at epoch 54: 0.000360
18/18 - 5s - 252ms/step - accuracy: 0.9554 - loss: 0.6981 - val_accuracy: 0.9750 - val_loss: 0.3150 - learning_rate: 3.6000e-04
Epoch 55/60
Learning rate at epoch 55: 0.000360
18/18 - 4s - 244ms/step - accuracy: 0.9571 - loss: 0.6833 - val_accuracy: 0.9750 - val_loss: 0.2844 - learning_rate: 3.6000e-04
Epoch 56/60
Learning rate at epoch 56: 0.000360
18/18 - 4s - 241ms/step - accuracy: 0.9554 - loss: 0.6177 - val_accuracy: 0.9833 - val_loss: 0.1710 - learning_rate: 3.6000e-04
Epoch 57/60
Learning rate at epoch 57: 0.000360
18/18 - 5s - 251ms/step - accuracy: 0.9482 - loss: 0.6651 - val_accuracy: 0.8167 - val_loss: 0.3221 - learning_rate: 3.6000e-04
Epoch 58/60
Learning rate at epoch 58: 0.000360
18/18 - 5s - 257ms/step - accuracy: 0.9607 - loss: 0.6368 - val_accuracy: 1.0000 - val_loss: 0.1526 - learning_rate: 3.6000e-04
Epoch 59/60
Learning rate at epoch 59: 0.000360
18/18 - 5s - 251ms/step - accuracy: 0.9589 - loss: 0.6184 - val_accuracy: 0.9917 - val_loss: 0.2072 - learning_rate: 3.6000e-04
Epoch 60/60
Learning rate at epoch 60: 0.000360
18/18 - 5s - 257ms/step - accuracy: 0.9607 - loss: 0.6102 - val_accuracy: 0.9917 - val_loss: 0.1778 - learning_rate: 3.6000e-04
Restoring model weights from the end of the best epoch: 58.

Best epoch: 58
Training  - Accuracy: 0.9607, Loss: 0.6368
Validation - Accuracy: 1.0000, Loss: 0.1526
```



```
# Cell 6: Evaluate via Balanced Accuracy & Confusion Matrix
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix, balanced_accuracy_score
import tensorflow as tf

@tf.function(reduce_retracing=True)
def predict_fn(x):
    return model(x, training=False)

y_pred = predict_fn(X_test).numpy().argmax(axis=1)
y_true = y_test

print("\nClassification Report:\n")
print(classification_report(y_true, y_pred, target_names=labels))
print(f"Balanced accuracy: {balanced_accuracy_score(y_true, y_pred):.3f}")

cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(4,3))
plt.imshow(cm, cmap="Blues")
plt.xticks(range(len(labels)), labels, rotation=45)
plt.yticks(range(len(labels)), labels)
plt.colorbar()
plt.title("Confusion Matrix")
plt.tight_layout()
plt.show()
```



```
Classification Report:

              precision    recall  f1-score   support

      Global       0.94      1.00      0.97        30
       Swiss       0.94      0.97      0.95        30
     Unknown       1.00      0.93      0.97        30
       Urban       0.97      0.93      0.95        30

    accuracy                           0.96       120
   macro avg       0.96      0.96      0.96       120
weighted avg       0.96      0.96      0.96       120

Balanced accuracy: 0.958

```




```
# Cell 7: Demo Interface via File Upload
import numpy as np
import librosa
import ipywidgets as widgets
import matplotlib.pyplot as plt
from IPython.display import display, Audio

# Upload widget for multiple files
uploader = widgets.FileUpload(
    accept='.wav,.mp3',
    multiple=True,
    description='Upload Cowbell Audio'
)
output = widgets.Output()

def preprocess_audio(data: bytes):
    # load bytes buffer with librosa
    import io
    y, _ = librosa.load(io.BytesIO(data), sr=sr, mono=True)
    if len(y) < n_samples:
        y = np.pad(y, (0, n_samples - len(y)))
    else:
        y = y[:n_samples]
    mel = extract_log_mel(y)
    return mel[..., np.newaxis]

def on_upload_change(change):
    with output:
        output.clear_output()
        for filename, fileinfo in uploader.value.items():
            print(f"=== {filename} ===")
            display(Audio(data=fileinfo['content'], rate=sr))
            mel = preprocess_audio(fileinfo['content'])
            probs = predict_fn(mel[np.newaxis, ...]).numpy()[0]
            for cls, p in zip(labels, probs):
                print(f"{cls}: {p:.4f}")
            # bar plot
            plt.figure(figsize=(5,2))
            plt.bar(labels, probs)
            plt.ylim(0,1)
            plt.title(f"Probabilities for {filename}")
            plt.xticks(rotation=45)
            plt.show()

uploader.observe(on_upload_change, names='value')
display(uploader, output)
```




```
# Cell 8: Mount Google Drive to Save Blog Post
from google.colab import drive
drive.mount('/content/drive')
```


```
@article{yiruyang2025,
  title={Pick Veo 3},
  author={Yiru Yang},
  journal={arXiv preprint arXiv:2506.xxxxx},
  year={2025}
}
```






