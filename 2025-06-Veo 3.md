
```
# Cell 1: Install only non-conflicting dependencies
!nvidia-smi
# Keep pandas and tensorflow at Colab defaults; install others
!pip install --upgrade --no-deps moviepy ipywidgets gdown librosa soundfile
!pip install comm
!pip install moviepy
```


```
# Cell 2: Download the 8 s silent video from Drive
import gdown, os

file_id = "11ds3js8chaDxicFnztKefvMPBjAouH8a"
gdown.download(
    f"https://drive.google.com/uc?export=download&id={file_id}",
    "video_silent.mp4",
    quiet=False
)
assert os.path.exists("video_silent.mp4"), "Failed to download video_silent.mp4"
print("Downloaded silent video")
```

```
%%bash
# Cell 3: Download ESC-50 and UrbanSound8K folds 1â€“3 only if not already present

# ESC-50
if [ ! -d "ESC-50-master" ]; then
  echo "Downloading ESC-50 dataset..."
  wget -q https://github.com/karolpiczak/ESC-50/archive/master.zip -O esc50.zip
  unzip -q esc50.zip
  rm esc50.zip
else
  echo "ESC-50 already exists â€” skipping."
fi

# UrbanSound8K folds 1â€“3 and metadata
if [ ! -d "UrbanSound8K/audio/fold1" ]; then
  echo "Downloading UrbanSound8K folds 1â€“3..."
  wget -q https://zenodo.org/record/1203745/files/UrbanSound8K.tar.gz -O us8k.tar.gz
  mkdir -p UrbanSound8K
  tar -xzf us8k.tar.gz \
      "UrbanSound8K/audio/fold1/" \
      "UrbanSound8K/audio/fold2/" \
      "UrbanSound8K/audio/fold3/" \
      "UrbanSound8K/metadata/UrbanSound8K.csv"
  rm us8k.tar.gz
else
  echo "UrbanSound8K folds 1â€“3 already exist â€” skipping."
fi
```

```
# Cell X: Inspect ESC-50 directory
!find ESC-50-master -maxdepth 2 -type f | head -n 20
!ls ESC-50-master/meta
!ls ESC-50-master/audio | head -n 5
!ls UrbanSound8K/audio/fold1 | head -n 5
!ls UrbanSound8K/audio/fold2 | head -n 5
!ls UrbanSound8K/audio/fold3 | head -n 5
!ls UrbanSound8K/metadata
```

```
ESC-50-master/.circleci/config.yml
ESC-50-master/.github/stale.yml
ESC-50-master/tests/test_dataset.py
ESC-50-master/audio/4-181707-A-32.wav
ESC-50-master/audio/3-148297-A-37.wav
ESC-50-master/audio/3-130998-B-28.wav
ESC-50-master/audio/3-119120-B-48.wav
ESC-50-master/audio/2-52085-B-4.wav
ESC-50-master/audio/1-196660-B-8.wav
ESC-50-master/audio/5-213836-A-9.wav
ESC-50-master/audio/5-233645-A-37.wav
ESC-50-master/audio/3-111102-B-46.wav
ESC-50-master/audio/3-100024-A-27.wav
ESC-50-master/audio/3-165856-A-41.wav
ESC-50-master/audio/5-180156-C-43.wav
ESC-50-master/audio/4-102844-A-49.wav
ESC-50-master/audio/5-234263-A-25.wav
ESC-50-master/audio/3-118194-A-33.wav
ESC-50-master/audio/4-135439-A-18.wav
ESC-50-master/audio/2-139748-B-15.wav
esc50.csv  esc50-human.xlsx
1-100032-A-0.wav
1-100038-A-14.wav
1-100210-A-36.wav
1-100210-B-36.wav
1-101296-A-19.wav
101415-3-0-2.wav
101415-3-0-3.wav
101415-3-0-8.wav
102106-3-0-0.wav
102305-6-0-0.wav
100652-3-0-0.wav
100652-3-0-1.wav
100652-3-0-2.wav
100652-3-0-3.wav
102104-3-0-0.wav
102105-3-0-0.wav
103199-4-0-0.wav
103199-4-0-3.wav
103199-4-0-4.wav
103199-4-0-5.wav
UrbanSound8K.csv
```


```
# Cell 4: Load, trim, and extract log-Mel features for three classes

import pandas as pd
import librosa
import numpy as np
import tensorflow as tf
import os

# Audio parameters
sr = 48000
duration = 5.0
max_len = int(sr * duration)

def load_and_trim(path):
    y, _ = librosa.load(path, sr=sr)
    if len(y) < max_len:
        y = np.tile(y, int(np.ceil(max_len / len(y))))[:max_len]
    else:
        y = y[:max_len]
    return y

def trim_audio(y):
    if len(y) < max_len:
        y = np.tile(y, int(np.ceil(max_len / len(y))))[:max_len]
    else:
        y = y[:max_len]
    return y


def extract_logmel(y):
    S = librosa.feature.melspectrogram(
        y=y, sr=sr, n_mels=64, n_fft=2048, hop_length=512
    )
    return librosa.power_to_db(S, ref=np.max)

X, y = [], []
labels = ["Swiss", "Urban", "Other"]

# 1) Swiss Alps: ESC-50 â€œcowâ€ class from CSV metadata
esc_meta_csv = pd.read_csv("ESC-50-master/meta/esc50.csv")
swiss_files = esc_meta_csv[esc_meta_csv.category == "cow"].filename.tolist()

for fn in swiss_files:
    path = f"ESC-50-master/audio/{fn}"
    if os.path.exists(path):
        wav = load_and_trim(path)
        X.append(extract_logmel(wav)); y.append(0)
print("Swiss samples:", sum(v == 0 for v in y))

# 2) Urban US: UrbanSound8K â€œbellâ€ classID=6, folds 1â€“3
urb_meta = pd.read_csv("UrbanSound8K/metadata/UrbanSound8K.csv")
bell = urb_meta[(urb_meta.classID == 6) & (urb_meta.fold <= 3)]
for _, r in bell.iterrows():
    path = f"UrbanSound8K/audio/fold{r.fold}/{r.slice_file_name}"
    if os.path.exists(path):
        wav = load_and_trim(path)
        X.append(extract_logmel(wav)); y.append(1)
print("Urban samples:", sum(v == 1 for v in y))

# 3) Other: augment first 40 Swiss cow samples
for fn in swiss_files[:40]:
    base = load_and_trim(f"ESC-50-master/audio/{fn}")
    for rate in (0.8, 1.2):
        aug = librosa.effects.time_stretch(y=base, rate=rate)
        trimmed = trim_audio(aug)
        X.append(extract_logmel(trimmed)); y.append(2)
    for n_steps in (2, -2):
        aug = librosa.effects.pitch_shift(y=base, sr=sr, n_steps=n_steps)
        trimmed = trim_audio(aug)
        X.append(extract_logmel(trimmed)); y.append(2)
print("Other samples:", sum(v == 2 for v in y))

# Stack features and one-hot encode labels
X = np.stack(X)[..., np.newaxis]
y_cat = tf.keras.utils.to_categorical(y, num_classes=3)
print("Dataset shapes:", X.shape, y_cat.shape)
```


```
Swiss samples: 40
Urban samples: 106
Other samples: 160
Dataset shapes: (306, 64, 469, 1) (306, 3)
```


```
# Cell 5: Build & train lightweight CNN + Adapter

from tensorflow.keras import layers, models

# Define model
inp = layers.Input(shape=X.shape[1:])
# Adapter block
x = layers.Conv2D(8, (1,1), activation="relu")(inp)
# Two convolutional layers
x = layers.Conv2D(16, (3,3), activation="relu", padding="same")(x)
x = layers.MaxPool2D()(x)
x = layers.Conv2D(32, (3,3), activation="relu", padding="same")(x)
x = layers.MaxPool2D()(x)
# Global pooling and dense layers
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dense(32, activation="relu")(x)
out = layers.Dense(3, activation="softmax")(x)

model = models.Model(inp, out)
model.compile(optimizer="adam",
              loss="categorical_crossentropy",
              metrics=["accuracy"])

# Train
model.fit(X, y_cat, epochs=20, batch_size=32)

print("Model trained")
```


```
Epoch 1/20
10/10 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 339ms/step - accuracy: 0.2949 - loss: 5.8424
Epoch 2/20
10/10 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3s 313ms/step - accuracy: 0.3962 - loss: 1.5525
Epoch 3/20
10/10 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3s 299ms/step - accuracy: 0.3622 - loss: 1.1074
Epoch 4/20
10/10 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3s 300ms/step - accuracy: 0.4436 - loss: 1.0159
Epoch 5/20
10/10 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3s 323ms/step - accuracy: 0.4975 - loss: 1.0029
Epoch 6/20
10/10 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 4s 363ms/step - accuracy: 0.5722 - loss: 0.9419
Epoch 7/20
10/10 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3s 304ms/step - accuracy: 0.5206 - loss: 0.9528
Epoch 8/20
10/10 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3s 303ms/step - accuracy: 0.6036 - loss: 0.9424
Epoch 9/20
10/10 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3s 328ms/step - accuracy: 0.6916 - loss: 0.8583
Epoch 10/20
10/10 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3s 338ms/step - accuracy: 0.6480 - loss: 0.8178
Epoch 11/20
10/10 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3s 330ms/step - accuracy: 0.7567 - loss: 0.7649
Epoch 12/20
10/10 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3s 325ms/step - accuracy: 0.8200 - loss: 0.6493
Epoch 13/20
10/10 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3s 330ms/step - accuracy: 0.7643 - loss: 0.6814
Epoch 14/20
10/10 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3s 311ms/step - accuracy: 0.8354 - loss: 0.5996
Epoch 15/20
10/10 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3s 322ms/step - accuracy: 0.8313 - loss: 0.5457
Epoch 16/20
10/10 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3s 310ms/step - accuracy: 0.8005 - loss: 0.5709
Epoch 17/20
10/10 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3s 331ms/step - accuracy: 0.8482 - loss: 0.5223
Epoch 18/20
10/10 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3s 310ms/step - accuracy: 0.8319 - loss: 0.5030
Epoch 19/20
10/10 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3s 306ms/step - accuracy: 0.8547 - loss: 0.4498
Epoch 20/20
10/10 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3s 312ms/step - accuracy: 0.8357 - loss: 0.4821
Model trained
```

```
# === Export TF-JS model & bundle resources ===
!pip install -q tensorflowjs

# 1. Convert .keras â†’ TF-JS LayersModel  (SINGLE line, no back-slashes!)
!tensorflowjs_converter --input_format=keras saved_cowbell.keras cowbell_web

# 2. Copy two 5-s sample WAVs for the demo
!cp ESC-50-master/audio/5-170338-B-41.wav swiss01.wav
!cp UrbanSound8K/audio/fold1/101415-3-0-2.wav urban01.wav
# (Optional) create other01.wav
!cp swiss01.wav other01.wav

# 3. Zip model + video + audio into one file
!zip -qr cowbell_site_bundle.zip cowbell_web video_silent.mp4 swiss01.wav urban01.wav other01.wav

# 4. Download to local machine
from google.colab import files
files.download("cowbell_site_bundle.zip")
```

```
2025-06-11 20:39:28.016788: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1749674368.043981   15273 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1749674368.051935   15273 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ðŸŒ² Try https://ydf.readthedocs.io, the successor of TensorFlow Decision Forests with more features and faster training!
Traceback (most recent call last):
  File "/usr/local/bin/tensorflowjs_converter", line 8, in <module>
    sys.exit(pip_main())
             ^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/tensorflowjs/converters/converter.py", line 959, in pip_main
    main([' '.join(sys.argv[1:])])
  File "/usr/local/lib/python3.11/dist-packages/tensorflowjs/converters/converter.py", line 963, in main
    convert(argv[0].split(' '))
  File "/usr/local/lib/python3.11/dist-packages/tensorflowjs/converters/converter.py", line 949, in convert
    _dispatch_converter(input_format, output_format, args, quantization_dtype_map,
  File "/usr/local/lib/python3.11/dist-packages/tensorflowjs/converters/converter.py", line 619, in _dispatch_converter
    dispatch_keras_h5_to_tfjs_layers_model_conversion(
  File "/usr/local/lib/python3.11/dist-packages/tensorflowjs/converters/converter.py", line 78, in dispatch_keras_h5_to_tfjs_layers_model_conversion
    raise ValueError('Nonexistent path to HDF5 file: %s' % h5_path)
ValueError: Nonexistent path to HDF5 file: saved_cowbell.keras
```


```
# Cell 6: Interactive demo: merge audio & classify

import ipywidgets as widgets
from IPython.display import Video, display
from moviepy.editor import VideoFileClip, AudioFileClip
import numpy as np # Import numpy here as well for indexing

# ensure esc_meta_csv, urb_meta, X, and model are still in scope
# sample paths (make sure 'esc_meta_csv' and 'urb_meta' exist from Cell 4)
# We no longer need the 'paths' list since we will get the path dynamically

dd = widgets.Dropdown(options=labels, description="Style:")
btn = widgets.Button(description="Merge & Classify")
out = widgets.Output()

def on_click(_):
    # ensure esc_meta_csv, urb_meta, X, model, and labels are accessible in this scope
    out.clear_output()
    idx = labels.index(dd.value)

    # Define index ranges based on the number of samples per category
    swiss_indices = np.arange(0, 40)
    urban_indices = np.arange(40, 40 + 106)
    other_indices = np.arange(40 + 106, 40 + 106 + 160) # Use actual counts from Cell 4 output

    if idx == 0: # Swiss
        pred_data = X[swiss_indices]
        sample_idx_in_X = np.random.choice(swiss_indices) # Select a random sample index within the Swiss range in X
        sample_for_pred = X[sample_idx_in_X:sample_idx_in_X+1] # Prepare sample for prediction

        # Find the original file path for the selected Swiss sample
        original_swiss_index = sample_idx_in_X
        original_file_name = esc_meta_csv[esc_meta_csv.category == 'cow'].filename.iloc[original_swiss_index]
        audio_path = f"ESC-50-master/audio/{original_file_name}"

    elif idx == 1: # Urban
        pred_data = X[urban_indices]
        sample_idx_in_X = np.random.choice(urban_indices) # Select a random sample index within the Urban range in X
        sample_for_pred = X[sample_idx_in_X:sample_idx_in_X+1] # Prepare sample for prediction

        # Find the original file path for the selected Urban sample
        original_urban_index = sample_idx_in_X - 40 # Adjust index to be relative to the start of Urban samples in X
        original_urban_row = urb_meta[(urb_meta.classID == 6) & (urb_meta.fold <= 3)].iloc[original_urban_index]
        audio_path = f"UrbanSound8K/audio/fold{original_urban_row.fold}/{original_urban_row.slice_file_name}"


    else: # Other
        pred_data = X[other_indices]
        sample_idx_in_X = np.random.choice(other_indices) # Select a random sample index within the Other range in X
        sample_for_pred = X[sample_idx_in_X:sample_idx_in_X+1] # Prepare sample for prediction

        # Finding the original file path for augmented "Other" samples is more complex.
        # Since "Other" samples are augmentations of the first 40 Swiss samples,
        # we can determine which of the original 40 Swiss files the augmented sample came from.
        # The 'Other' samples in X are structured as: 4 augmentations for swiss_files[0],
        # then 4 for swiss_files[1], and so on, up to swiss_files[39].
        original_swiss_index_for_augmentation = (sample_idx_in_X - (40 + 106)) // 4 # Adjust index and find which original Swiss file it was based on groups of 4 augmentations
        original_file_name = esc_meta_csv[esc_meta_csv.category == 'cow'].filename.iloc[original_swiss_index_for_augmentation]
        audio_path = f"ESC-50-master/audio/{original_file_name}"


    clip = VideoFileClip("video_silent.mp4").set_audio(AudioFileClip(audio_path))
    clip.write_videofile("demo.mp4", verbose=False, logger=None)
    # Use the selected sample for prediction
    pred = model.predict(sample_for_pred)[0]
    with out:
        display(Video("demo.mp4", embed=True))
        print("Prediction:", dict(zip(labels, pred)))

btn.on_click(on_click)
display(widgets.VBox([dd, btn, out]))
```

```

```

