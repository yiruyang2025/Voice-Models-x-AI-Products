
```
# Cell 1: Install only non-conflicting dependencies
!nvidia-smi
# Keep pandas and tensorflow at Colab defaults; install others
!pip install --upgrade --no-deps moviepy ipywidgets gdown librosa soundfile
```


```
# Cell 2: Download the 8 s silent video from Drive
import gdown, os

file_id = "11ds3js8chaDxicFnztKefvMPBjAouH8a"
gdown.download(
    f"https://drive.google.com/uc?export=download&id={file_id}",
    "video_silent.mp4",
    quiet=False
)
assert os.path.exists("video_silent.mp4"), "Failed to download video_silent.mp4"
print("Downloaded silent video")
```

```
%%bash
# Cell 3: Download ESC-50 and UrbanSound8K folds 1–3 only if not already present

# ESC-50
if [ ! -d "ESC-50-master" ]; then
  echo "Downloading ESC-50 dataset..."
  wget -q https://github.com/karolpiczak/ESC-50/archive/master.zip -O esc50.zip
  unzip -q esc50.zip
  rm esc50.zip
else
  echo "ESC-50 already exists — skipping."
fi

# UrbanSound8K folds 1–3 and metadata
if [ ! -d "UrbanSound8K/audio/fold1" ]; then
  echo "Downloading UrbanSound8K folds 1–3..."
  wget -q https://zenodo.org/record/1203745/files/UrbanSound8K.tar.gz -O us8k.tar.gz
  mkdir -p UrbanSound8K
  tar -xzf us8k.tar.gz \
      "UrbanSound8K/audio/fold1/" \
      "UrbanSound8K/audio/fold2/" \
      "UrbanSound8K/audio/fold3/" \
      "UrbanSound8K/metadata/UrbanSound8K.csv"
  rm us8k.tar.gz
else
  echo "UrbanSound8K folds 1–3 already exist — skipping."
fi
```

```
# Cell X: Inspect ESC-50 directory
!find ESC-50-master -maxdepth 2 -type f | head -n 20
!ls ESC-50-master/meta
```

```
ESC-50-master/README.md
ESC-50-master/audio/5-170338-B-41.wav
ESC-50-master/audio/4-99193-A-4.wav
ESC-50-master/audio/2-196688-A-8.wav
ESC-50-master/audio/1-81851-A-31.wav
ESC-50-master/audio/2-106487-A-44.wav
ESC-50-master/audio/4-261068-A-30.wav
ESC-50-master/audio/4-128659-A-33.wav
ESC-50-master/audio/2-114609-B-28.wav
ESC-50-master/audio/4-198965-A-38.wav
ESC-50-master/audio/5-242490-A-14.wav
ESC-50-master/audio/3-129338-A-13.wav
ESC-50-master/audio/4-119648-C-48.wav
ESC-50-master/audio/5-213836-D-9.wav
ESC-50-master/audio/5-204114-A-29.wav
ESC-50-master/audio/1-24524-C-19.wav
ESC-50-master/audio/5-260164-A-23.wav
ESC-50-master/audio/4-177243-A-32.wav
ESC-50-master/audio/5-197913-A-18.wav
ESC-50-master/audio/2-78562-B-37.wav
esc50.csv  esc50-human.xlsx
```


```
# Cell 4: Load, trim, and extract log-Mel features for three classes

import pandas as pd
import librosa
import numpy as np
import tensorflow as tf
import os

# Audio parameters
sr = 48000
duration = 5.0
max_len = int(sr * duration)

def load_and_trim(path):
    y, _ = librosa.load(path, sr=sr)
    if len(y) < max_len:
        y = np.tile(y, int(np.ceil(max_len / len(y))))[:max_len]
    else:
        y = y[:max_len]
    return y

def trim_audio(y):
    if len(y) < max_len:
        y = np.tile(y, int(np.ceil(max_len / len(y))))[:max_len]
    else:
        y = y[:max_len]
    return y


def extract_logmel(y):
    S = librosa.feature.melspectrogram(
        y=y, sr=sr, n_mels=64, n_fft=2048, hop_length=512
    )
    return librosa.power_to_db(S, ref=np.max)

X, y = [], []
labels = ["Swiss", "Urban", "Other"]

# 1) Swiss Alps: ESC-50 “cow” class from CSV metadata
esc_meta_csv = pd.read_csv("ESC-50-master/meta/esc50.csv")
swiss_files = esc_meta_csv[esc_meta_csv.category == "cow"].filename.tolist()

for fn in swiss_files:
    path = f"ESC-50-master/audio/{fn}"
    if os.path.exists(path):
        wav = load_and_trim(path)
        X.append(extract_logmel(wav)); y.append(0)
print("Swiss samples:", sum(v == 0 for v in y))

# 2) Urban US: UrbanSound8K “bell” classID=6, folds 1–3
urb_meta = pd.read_csv("UrbanSound8K/metadata/UrbanSound8K.csv")
bell = urb_meta[(urb_meta.classID == 6) & (urb_meta.fold <= 3)]
for _, r in bell.iterrows():
    path = f"UrbanSound8K/audio/fold{r.fold}/{r.slice_file_name}"
    if os.path.exists(path):
        wav = load_and_trim(path)
        X.append(extract_logmel(wav)); y.append(1)
print("Urban samples:", sum(v == 1 for v in y))

# 3) Other: augment first 40 Swiss cow samples
for fn in swiss_files[:40]:
    base = load_and_trim(f"ESC-50-master/audio/{fn}")
    for rate in (0.8, 1.2):
        aug = librosa.effects.time_stretch(y=base, rate=rate)
        trimmed = trim_audio(aug)
        X.append(extract_logmel(trimmed)); y.append(2)
    for n_steps in (2, -2):
        aug = librosa.effects.pitch_shift(y=base, sr=sr, n_steps=n_steps)
        trimmed = trim_audio(aug)
        X.append(extract_logmel(trimmed)); y.append(2)
print("Other samples:", sum(v == 2 for v in y))

# Stack features and one-hot encode labels
X = np.stack(X)[..., np.newaxis]
y_cat = tf.keras.utils.to_categorical(y, num_classes=3)
print("Dataset shapes:", X.shape, y_cat.shape)
```


```
Swiss samples: 40
Urban samples: 106
Other samples: 160
Dataset shapes: (306, 64, 469, 1) (306, 3)
```


```
# Cell 5: Build & train lightweight CNN + Adapter
from tensorflow.keras import layers, models

inp = layers.Input(shape=X.shape[1:])
x = layers.Conv2D(8,(1,1),activation="relu")(inp)
x = layers.Conv2D(16,(3,3),activation="relu",padding="same")(x)
x = layers.MaxPool2D()(x)
x = layers.Conv2D(32,(3,3),activation="relu",padding="same")(x)
x = layers.MaxPool2D()(x)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dense(32,activation="relu")(x)
out = layers.Dense(3,activation="softmax")(x)

model = models.Model(inp,out)
model.compile("adam","categorical_crossentropy",["accuracy"])
model.fit(X,y_cat,epochs=20,batch_size=32)
print("Model trained")
```



```
# Cell 6: Interactive demo: merge audio & classify
import ipywidgets as widgets
from IPython.display import Video, display
from moviepy.editor import VideoFileClip, AudioFileClip

# sample paths
swiss = f"ESC-50-master/audio/{esc.filename.iloc[0]}"
urban = f"UrbanSound8K/audio/fold1/{bell.slice_file_name.iloc[0]}"
other = swiss

paths = [swiss,urban,other]
dd = widgets.Dropdown(options=labels,description="Style:")
btn = widgets.Button(description="Merge & Classify")
out = widgets.Output()

def on_click(_):
    out.clear_output()
    idx = labels.index(dd.value)
    clip = VideoFileClip("video_silent.mp4").set_audio(AudioFileClip(paths[idx]))
    clip.write_videofile("demo.mp4",verbose=False,logger=None)
    pred = model.predict(X[idx:idx+1])[0]
    with out:
        display(Video("demo.mp4",embed=True))
        print("Prediction:", dict(zip(labels,pred)))

btn.on_click(on_click)
display(widgets.VBox([dd,btn,out]))
```



