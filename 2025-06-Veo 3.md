
```
# Cell 1: Install only non-conflicting dependencies
!nvidia-smi
# Keep pandas and tensorflow at Colab defaults; install others
!pip install --upgrade --no-deps moviepy ipywidgets gdown librosa soundfile
!pip install comm
!pip install moviepy
```


```
# Cell 2: Download the 8 s silent video from Drive
import gdown, os

file_id = "11ds3js8chaDxicFnztKefvMPBjAouH8a"
gdown.download(
    f"https://drive.google.com/uc?export=download&id={file_id}",
    "video_silent.mp4",
    quiet=False
)
assert os.path.exists("video_silent.mp4"), "Failed to download video_silent.mp4"
print("Downloaded silent video")
```

```
%%bash
# Cell 3: Download ESC-50 and UrbanSound8K folds 1–3 only if not already present

# ESC-50
if [ ! -d "ESC-50-master" ]; then
  echo "Downloading ESC-50 dataset..."
  wget -q https://github.com/karolpiczak/ESC-50/archive/master.zip -O esc50.zip
  unzip -q esc50.zip
  rm esc50.zip
else
  echo "ESC-50 already exists — skipping."
fi

# UrbanSound8K folds 1–3 and metadata
if [ ! -d "UrbanSound8K/audio/fold1" ]; then
  echo "Downloading UrbanSound8K folds 1–3..."
  wget -q https://zenodo.org/record/1203745/files/UrbanSound8K.tar.gz -O us8k.tar.gz
  mkdir -p UrbanSound8K
  tar -xzf us8k.tar.gz \
      "UrbanSound8K/audio/fold1/" \
      "UrbanSound8K/audio/fold2/" \
      "UrbanSound8K/audio/fold3/" \
      "UrbanSound8K/metadata/UrbanSound8K.csv"
  rm us8k.tar.gz
else
  echo "UrbanSound8K folds 1–3 already exist — skipping."
fi
```

```
# Cell X: Inspect ESC-50 directory
!find ESC-50-master -maxdepth 2 -type f | head -n 20
!ls ESC-50-master/meta
!ls ESC-50-master/audio | head -n 5
!ls UrbanSound8K/audio/fold1 | head -n 5
!ls UrbanSound8K/audio/fold2 | head -n 5
!ls UrbanSound8K/audio/fold3 | head -n 5
!ls UrbanSound8K/metadata
```

```
ESC-50-master/.circleci/config.yml
ESC-50-master/.github/stale.yml
ESC-50-master/tests/test_dataset.py
ESC-50-master/audio/4-181707-A-32.wav
ESC-50-master/audio/3-148297-A-37.wav
ESC-50-master/audio/3-130998-B-28.wav
ESC-50-master/audio/3-119120-B-48.wav
ESC-50-master/audio/2-52085-B-4.wav
ESC-50-master/audio/1-196660-B-8.wav
ESC-50-master/audio/5-213836-A-9.wav
ESC-50-master/audio/5-233645-A-37.wav
ESC-50-master/audio/3-111102-B-46.wav
ESC-50-master/audio/3-100024-A-27.wav
ESC-50-master/audio/3-165856-A-41.wav
ESC-50-master/audio/5-180156-C-43.wav
ESC-50-master/audio/4-102844-A-49.wav
ESC-50-master/audio/5-234263-A-25.wav
ESC-50-master/audio/3-118194-A-33.wav
ESC-50-master/audio/4-135439-A-18.wav
ESC-50-master/audio/2-139748-B-15.wav
esc50.csv  esc50-human.xlsx
1-100032-A-0.wav
1-100038-A-14.wav
1-100210-A-36.wav
1-100210-B-36.wav
1-101296-A-19.wav
101415-3-0-2.wav
101415-3-0-3.wav
101415-3-0-8.wav
102106-3-0-0.wav
102305-6-0-0.wav
100652-3-0-0.wav
100652-3-0-1.wav
100652-3-0-2.wav
100652-3-0-3.wav
102104-3-0-0.wav
102105-3-0-0.wav
103199-4-0-0.wav
103199-4-0-3.wav
103199-4-0-4.wav
103199-4-0-5.wav
UrbanSound8K.csv
```


```
# Cell 4: Load, trim, and extract log-Mel features for three classes

import pandas as pd
import librosa
import numpy as np
import tensorflow as tf
import os

# Audio parameters
sr = 48000
duration = 5.0
max_len = int(sr * duration)

def load_and_trim(path):
    y, _ = librosa.load(path, sr=sr)
    if len(y) < max_len:
        y = np.tile(y, int(np.ceil(max_len / len(y))))[:max_len]
    else:
        y = y[:max_len]
    return y

def trim_audio(y):
    if len(y) < max_len:
        y = np.tile(y, int(np.ceil(max_len / len(y))))[:max_len]
    else:
        y = y[:max_len]
    return y


def extract_logmel(y):
    S = librosa.feature.melspectrogram(
        y=y, sr=sr, n_mels=64, n_fft=2048, hop_length=512
    )
    return librosa.power_to_db(S, ref=np.max)

X, y = [], []
labels = ["Swiss", "Urban", "Other"]

# 1) Swiss Alps: ESC-50 “cow” class from CSV metadata
esc_meta_csv = pd.read_csv("ESC-50-master/meta/esc50.csv")
swiss_files = esc_meta_csv[esc_meta_csv.category == "cow"].filename.tolist()

for fn in swiss_files:
    path = f"ESC-50-master/audio/{fn}"
    if os.path.exists(path):
        wav = load_and_trim(path)
        X.append(extract_logmel(wav)); y.append(0)
print("Swiss samples:", sum(v == 0 for v in y))

# 2) Urban US: UrbanSound8K “bell” classID=6, folds 1–3
urb_meta = pd.read_csv("UrbanSound8K/metadata/UrbanSound8K.csv")
bell = urb_meta[(urb_meta.classID == 6) & (urb_meta.fold <= 3)]
for _, r in bell.iterrows():
    path = f"UrbanSound8K/audio/fold{r.fold}/{r.slice_file_name}"
    if os.path.exists(path):
        wav = load_and_trim(path)
        X.append(extract_logmel(wav)); y.append(1)
print("Urban samples:", sum(v == 1 for v in y))

# 3) Other: augment first 40 Swiss cow samples
for fn in swiss_files[:40]:
    base = load_and_trim(f"ESC-50-master/audio/{fn}")
    for rate in (0.8, 1.2):
        aug = librosa.effects.time_stretch(y=base, rate=rate)
        trimmed = trim_audio(aug)
        X.append(extract_logmel(trimmed)); y.append(2)
    for n_steps in (2, -2):
        aug = librosa.effects.pitch_shift(y=base, sr=sr, n_steps=n_steps)
        trimmed = trim_audio(aug)
        X.append(extract_logmel(trimmed)); y.append(2)
print("Other samples:", sum(v == 2 for v in y))

# Stack features and one-hot encode labels
X = np.stack(X)[..., np.newaxis]
y_cat = tf.keras.utils.to_categorical(y, num_classes=3)
print("Dataset shapes:", X.shape, y_cat.shape)
```


```
Swiss samples: 40
Urban samples: 106
Other samples: 160
Dataset shapes: (306, 64, 469, 1) (306, 3)
```


```
# Cell 5 — Build, train and validate Lightweight CNN + Adapter

import numpy as np
from tensorflow.keras import layers, models, callbacks, optimizers
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt

# ── 0. DATA SPLIT (70 / 15 / 15) ──────────────────────────────────────────────
# y_class for stratification
y_class = y_cat.argmax(axis=1)

X_train, X_tmp, y_train, y_tmp = train_test_split(
    X, y_cat, test_size=0.30, stratify=y_class, random_state=42)

y_tmp_class = y_tmp.argmax(axis=1)
X_val, X_test, y_val, y_test = train_test_split(
    X_tmp, y_tmp, test_size=0.50, stratify=y_tmp_class, random_state=42)

print("Train/Val/Test shapes:", X_train.shape, X_val.shape, X_test.shape)

# ── 1. Model definition ───────────────────────────────────────────────────────
inp = layers.Input(shape=X.shape[1:])  # (64, 469, 1)

x = layers.Conv2D(8, (1, 1), activation="relu")(inp)          # adapter
x = layers.Conv2D(16, (3, 3), activation="relu", padding="same")(x)
x = layers.MaxPool2D()(x)
x = layers.Conv2D(32, (3, 3), activation="relu", padding="same")(x)
x = layers.MaxPool2D()(x)

x = layers.GlobalAveragePooling2D()(x)
x = layers.Dense(32, activation="relu")(x)
out = layers.Dense(3, activation="softmax")(x)

model = models.Model(inp, out)
model.compile(
    optimizer=optimizers.Adam(1e-3, weight_decay=1e-4),  # slight L2
    loss="categorical_crossentropy",
    metrics=["accuracy"]
)

# ── 2. Callback: EarlyStopping on validation accuracy ─────────────────────────
early_stop = callbacks.EarlyStopping(
    monitor="val_accuracy",
    patience=5,
    restore_best_weights=True
)

# ── 3. Train ──────────────────────────────────────────────────────────────────
history = model.fit(
    X_train, y_train,
    epochs=60,
    batch_size=32,
    validation_data=(X_val, y_val),
    callbacks=[early_stop],
    verbose=2
)

print("Best val_accuracy:", max(history.history["val_accuracy"]))

# ── 4. Evaluate on independent test set ───────────────────────────────────────
test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
print(f"Test accuracy: {test_acc:.3f}")

# ── 5. Confusion matrix on test set ───────────────────────────────────────────
y_pred = model.predict(X_test).argmax(axis=1)
y_true = y_test.argmax(axis=1)
cm = confusion_matrix(y_true, y_pred)

print("\nConfusion matrix (rows=true, cols=pred):\n", cm)
print("\nClassification report:\n",
      classification_report(y_true, y_pred, target_names=labels))

# Optional: quick heat-map plot
plt.figure(figsize=(4, 3))
plt.imshow(cm, cmap="Blues")
plt.xticks(range(3), labels, rotation=45)
plt.yticks(range(3), labels)
plt.colorbar(); plt.title("Test Confusion Matrix")
plt.xlabel("Predicted"); plt.ylabel("True")
plt.show()
```


```

```

```

```

```

```


```
# Cell 6: Interactive demo: merge audio & classify

import ipywidgets as widgets
from IPython.display import Video, display
from moviepy.editor import VideoFileClip, AudioFileClip
import numpy as np # Import numpy here as well for indexing

# ensure esc_meta_csv, urb_meta, X, and model are still in scope
# sample paths (make sure 'esc_meta_csv' and 'urb_meta' exist from Cell 4)
# We no longer need the 'paths' list since we will get the path dynamically

dd = widgets.Dropdown(options=labels, description="Style:")
btn = widgets.Button(description="Merge & Classify")
out = widgets.Output()

def on_click(_):
    # ensure esc_meta_csv, urb_meta, X, model, and labels are accessible in this scope
    out.clear_output()
    idx = labels.index(dd.value)

    # Define index ranges based on the number of samples per category
    swiss_indices = np.arange(0, 40)
    urban_indices = np.arange(40, 40 + 106)
    other_indices = np.arange(40 + 106, 40 + 106 + 160) # Use actual counts from Cell 4 output

    if idx == 0: # Swiss
        pred_data = X[swiss_indices]
        sample_idx_in_X = np.random.choice(swiss_indices) # Select a random sample index within the Swiss range in X
        sample_for_pred = X[sample_idx_in_X:sample_idx_in_X+1] # Prepare sample for prediction

        # Find the original file path for the selected Swiss sample
        original_swiss_index = sample_idx_in_X
        original_file_name = esc_meta_csv[esc_meta_csv.category == 'cow'].filename.iloc[original_swiss_index]
        audio_path = f"ESC-50-master/audio/{original_file_name}"

    elif idx == 1: # Urban
        pred_data = X[urban_indices]
        sample_idx_in_X = np.random.choice(urban_indices) # Select a random sample index within the Urban range in X
        sample_for_pred = X[sample_idx_in_X:sample_idx_in_X+1] # Prepare sample for prediction

        # Find the original file path for the selected Urban sample
        original_urban_index = sample_idx_in_X - 40 # Adjust index to be relative to the start of Urban samples in X
        original_urban_row = urb_meta[(urb_meta.classID == 6) & (urb_meta.fold <= 3)].iloc[original_urban_index]
        audio_path = f"UrbanSound8K/audio/fold{original_urban_row.fold}/{original_urban_row.slice_file_name}"


    else: # Other
        pred_data = X[other_indices]
        sample_idx_in_X = np.random.choice(other_indices) # Select a random sample index within the Other range in X
        sample_for_pred = X[sample_idx_in_X:sample_idx_in_X+1] # Prepare sample for prediction

        # Finding the original file path for augmented "Other" samples is more complex.
        # Since "Other" samples are augmentations of the first 40 Swiss samples,
        # we can determine which of the original 40 Swiss files the augmented sample came from.
        # The 'Other' samples in X are structured as: 4 augmentations for swiss_files[0],
        # then 4 for swiss_files[1], and so on, up to swiss_files[39].
        original_swiss_index_for_augmentation = (sample_idx_in_X - (40 + 106)) // 4 # Adjust index and find which original Swiss file it was based on groups of 4 augmentations
        original_file_name = esc_meta_csv[esc_meta_csv.category == 'cow'].filename.iloc[original_swiss_index_for_augmentation]
        audio_path = f"ESC-50-master/audio/{original_file_name}"


    clip = VideoFileClip("video_silent.mp4").set_audio(AudioFileClip(audio_path))
    clip.write_videofile("demo.mp4", verbose=False, logger=None)
    # Use the selected sample for prediction
    pred = model.predict(sample_for_pred)[0]
    with out:
        display(Video("demo.mp4", embed=True))
        print("Prediction:", dict(zip(labels, pred)))

btn.on_click(on_click)
display(widgets.VBox([dd, btn, out]))
```

```

```

