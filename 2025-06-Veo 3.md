
```
# Cell 1: Install only non-conflicting dependencies
!nvidia-smi
# Keep pandas and tensorflow at Colab defaults; install others
!pip install --upgrade --no-deps moviepy ipywidgets gdown librosa soundfile
!pip install comm
!pip install moviepy
```


```
# Cell 2: Download the 8 s silent video from Drive
import gdown, os

file_id = "11ds3js8chaDxicFnztKefvMPBjAouH8a"
gdown.download(
    f"https://drive.google.com/uc?export=download&id={file_id}",
    "video_silent.mp4",
    quiet=False
)
assert os.path.exists("video_silent.mp4"), "Failed to download video_silent.mp4"
print("Downloaded silent video")
```

```
%%bash
# Cell 3: Download ESC-50 and UrbanSound8K folds 1–3 only if not already present

# ESC-50
if [ ! -d "ESC-50-master" ]; then
  echo "Downloading ESC-50 dataset..."
  wget -q https://github.com/karolpiczak/ESC-50/archive/master.zip -O esc50.zip
  unzip -q esc50.zip
  rm esc50.zip
else
  echo "ESC-50 already exists — skipping."
fi

# UrbanSound8K folds 1–3 and metadata
if [ ! -d "UrbanSound8K/audio/fold1" ]; then
  echo "Downloading UrbanSound8K folds 1–3..."
  wget -q https://zenodo.org/record/1203745/files/UrbanSound8K.tar.gz -O us8k.tar.gz
  mkdir -p UrbanSound8K
  tar -xzf us8k.tar.gz \
      "UrbanSound8K/audio/fold1/" \
      "UrbanSound8K/audio/fold2/" \
      "UrbanSound8K/audio/fold3/" \
      "UrbanSound8K/metadata/UrbanSound8K.csv"
  rm us8k.tar.gz
else
  echo "UrbanSound8K folds 1–3 already exist — skipping."
fi
```

```
# Cell X: Inspect ESC-50 directory
!find ESC-50-master -maxdepth 2 -type f | head -n 20
!ls ESC-50-master/meta
!ls ESC-50-master/audio | head -n 5
!ls UrbanSound8K/audio/fold1 | head -n 5
!ls UrbanSound8K/audio/fold2 | head -n 5
!ls UrbanSound8K/audio/fold3 | head -n 5
!ls UrbanSound8K/metadata
```

```

```


```
# Cell 4: Load, trim, and extract log-Mel features for three classes

import pandas as pd
import librosa
import numpy as np
import tensorflow as tf
import os

# Audio parameters
sr = 48000
duration = 5.0
max_len = int(sr * duration)

def load_and_trim(path):
    y, _ = librosa.load(path, sr=sr)
    if len(y) < max_len:
        y = np.tile(y, int(np.ceil(max_len / len(y))))[:max_len]
    else:
        y = y[:max_len]
    return y

def trim_audio(y):
    if len(y) < max_len:
        y = np.tile(y, int(np.ceil(max_len / len(y))))[:max_len]
    else:
        y = y[:max_len]
    return y


def extract_logmel(y):
    S = librosa.feature.melspectrogram(
        y=y, sr=sr, n_mels=64, n_fft=2048, hop_length=512
    )
    return librosa.power_to_db(S, ref=np.max)

X, y = [], []
labels = ["Swiss", "Urban", "Other"]

# 1) Swiss Alps: ESC-50 “cow” class from CSV metadata
esc_meta_csv = pd.read_csv("ESC-50-master/meta/esc50.csv")
swiss_files = esc_meta_csv[esc_meta_csv.category == "cow"].filename.tolist()

for fn in swiss_files:
    path = f"ESC-50-master/audio/{fn}"
    if os.path.exists(path):
        wav = load_and_trim(path)
        X.append(extract_logmel(wav)); y.append(0)
print("Swiss samples:", sum(v == 0 for v in y))

# 2) Urban US: UrbanSound8K “bell” classID=6, folds 1–3
urb_meta = pd.read_csv("UrbanSound8K/metadata/UrbanSound8K.csv")
bell = urb_meta[(urb_meta.classID == 6) & (urb_meta.fold <= 3)]
for _, r in bell.iterrows():
    path = f"UrbanSound8K/audio/fold{r.fold}/{r.slice_file_name}"
    if os.path.exists(path):
        wav = load_and_trim(path)
        X.append(extract_logmel(wav)); y.append(1)
print("Urban samples:", sum(v == 1 for v in y))

# 3) Other: augment first 40 Swiss cow samples
for fn in swiss_files[:40]:
    base = load_and_trim(f"ESC-50-master/audio/{fn}")
    for rate in (0.8, 1.2):
        aug = librosa.effects.time_stretch(y=base, rate=rate)
        trimmed = trim_audio(aug)
        X.append(extract_logmel(trimmed)); y.append(2)
    for n_steps in (2, -2):
        aug = librosa.effects.pitch_shift(y=base, sr=sr, n_steps=n_steps)
        trimmed = trim_audio(aug)
        X.append(extract_logmel(trimmed)); y.append(2)
print("Other samples:", sum(v == 2 for v in y))

# Stack features and one-hot encode labels
X = np.stack(X)[..., np.newaxis]
y_cat = tf.keras.utils.to_categorical(y, num_classes=3)
print("Dataset shapes:", X.shape, y_cat.shape)
```


```
Swiss samples: 40
Urban samples: 106
Other samples: 160
Dataset shapes: (306, 64, 469, 1) (306, 3)
```


```
# Cell 5: Build & train lightweight CNN + Adapter

from tensorflow.keras import layers, models

# Define model
inp = layers.Input(shape=X.shape[1:])
# Adapter block
x = layers.Conv2D(8, (1,1), activation="relu")(inp)
# Two convolutional layers
x = layers.Conv2D(16, (3,3), activation="relu", padding="same")(x)
x = layers.MaxPool2D()(x)
x = layers.Conv2D(32, (3,3), activation="relu", padding="same")(x)
x = layers.MaxPool2D()(x)
# Global pooling and dense layers
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dense(32, activation="relu")(x)
out = layers.Dense(3, activation="softmax")(x)

model = models.Model(inp, out)
model.compile(optimizer="adam",
              loss="categorical_crossentropy",
              metrics=["accuracy"])

# Train
model.fit(X, y_cat, epochs=20, batch_size=32)

print("Model trained")
```


```
Epoch 1/20
10/10 ━━━━━━━━━━━━━━━━━━━━ 8s 284ms/step - accuracy: 0.3526 - loss: 3.0631
Epoch 2/20
10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4608 - loss: 1.1618 
Epoch 3/20
10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4520 - loss: 1.0594 
Epoch 4/20
10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4844 - loss: 1.0329 
Epoch 5/20
10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.5344 - loss: 0.9791 
Epoch 6/20
10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.5744 - loss: 0.9875 
Epoch 7/20
10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.5690 - loss: 0.9677 
Epoch 8/20
10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.5522 - loss: 0.9152
Epoch 9/20
10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.6335 - loss: 0.9324
Epoch 10/20
10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.7181 - loss: 0.8944
Epoch 11/20
10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.5987 - loss: 0.8452
Epoch 12/20
10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step - accuracy: 0.6296 - loss: 0.8650
Epoch 13/20
10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.6746 - loss: 0.8419
Epoch 14/20
10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.6907 - loss: 0.7967
Epoch 15/20
10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.7852 - loss: 0.7610
Epoch 16/20
10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.6981 - loss: 0.7726
Epoch 17/20
10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.8326 - loss: 0.6210
Epoch 18/20
10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.8193 - loss: 0.5731
Epoch 19/20
10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.7691 - loss: 0.6490
Epoch 20/20
10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.8296 - loss: 0.5773
Model trained
```


```
# Cell 6: Interactive demo: merge audio & classify
import ipywidgets as widgets
from IPython.display import Video, display
from moviepy.editor import VideoFileClip, AudioFileClip

# sample paths
swiss = f"ESC-50-master/audio/{esc.filename.iloc[0]}"
urban = f"UrbanSound8K/audio/fold1/{bell.slice_file_name.iloc[0]}"
other = swiss

paths = [swiss,urban,other]
dd = widgets.Dropdown(options=labels,description="Style:")
btn = widgets.Button(description="Merge & Classify")
out = widgets.Output()

def on_click(_):
    out.clear_output()
    idx = labels.index(dd.value)
    clip = VideoFileClip("video_silent.mp4").set_audio(AudioFileClip(paths[idx]))
    clip.write_videofile("demo.mp4",verbose=False,logger=None)
    pred = model.predict(X[idx:idx+1])[0]
    with out:
        display(Video("demo.mp4",embed=True))
        print("Prediction:", dict(zip(labels,pred)))

btn.on_click(on_click)
display(widgets.VBox([dd,btn,out]))
```



